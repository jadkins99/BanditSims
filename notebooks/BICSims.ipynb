{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from bandits.environment import Environment\n",
    "from bandits.bandit import GaussianBandit\n",
    "from bandits.bandit import BernoulliBandit\n",
    "from bandits.bandit import TruncatedGaussianBandit\n",
    "import math\n",
    "from bandits.agent import Agent, GradientAgent\n",
    "from bandits.policy import (EpsilonGreedyPolicy, GreedyPolicy, UCBPolicy,\n",
    "                            ExploreFirstPolicy,\n",
    "                            SoftmaxPolicy)\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from bandits.agent import Agent, BetaAgent\n",
    "import seaborn as sns\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "num_bandit_rounds = 20000\n",
    "np.random.seed(3)\n",
    "\n",
    "\n",
    "p_array = []\n",
    "\n",
    "p_expected_values = np.linspace(start = 0.9,stop = 0.1, num = n_arms)\n",
    "\n",
    "for i in p_expected_values:\n",
    "    p_array.append(pm.TruncatedNormal.dist(lower=0,upper=1,mu=i,sigma = 0.4).random())\n",
    "    \n",
    "    \n",
    "\n",
    "p_best = np.max(p_array)\n",
    "p_worst = np.min(p_array)\n",
    "\n",
    "\n",
    "\n",
    "bandit = BernoulliBandit(k=n_arms,p_array = p_array)\n",
    "rho_p = 1/n_arms\n",
    "tau_p = 1/n_arms\n",
    "\n",
    "L_p  = int(2 + (p_best - p_worst)/(rho_p*tau_p))\n",
    "k_p = 1000\n",
    "\n",
    "print(p_array)\n",
    "print(k_p)\n",
    "print(L_p)\n",
    "print(L_p*num_bandit_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420580cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BICAgent:\n",
    "    def __init__(self, k_p,L_p,K):\n",
    "        self.k_p = k_p\n",
    "        self. L_p = L_p\n",
    "        self.K = K\n",
    "        self.model = pm.Model()\n",
    "        self._value_estimates = np.zeros(self.K)\n",
    "        self.a_star = 0\n",
    "        self.alpha = np.ones(self.K)\n",
    "        self.beta = np.ones(self.K)\n",
    "\n",
    "\n",
    "    def reset():\n",
    "        self._value_estimates = np.zeros(self.K)\n",
    "        self.a_star = 0\n",
    "        self.alphas = np.ones(self.K)\n",
    "        self.betas = np.ones(self.K)\n",
    "        \n",
    "    \n",
    "    def update(self,i,reward):\n",
    "        self.alpha[i] += reward\n",
    "        self.beta[i] += 1 - reward\n",
    "        self._value_estimates = self.alpha / (self.alpha + self.beta)\n",
    "        self.a_star =  np.argmax(self._value_estimates)\n",
    "        \n",
    "  \n",
    "    \n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_num_agents = 0\n",
    "regret = []\n",
    "\n",
    "\n",
    "r_1 = np.repeat(-1,k_p)\n",
    "# sampling stage\n",
    "\n",
    "agent = BICAgent(k_p,L_p,n_arms)\n",
    "# sample k_p times from bandit 1\n",
    "for i in range(k_p):\n",
    "    sampling_num_agents += 1\n",
    "    r_1[i] = bandit.pull(0)[0]\n",
    "    agent.update(0,r_1[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(1,n_arms):\n",
    "    \n",
    "    a_star = agent.a_star\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    explore_agents = set(np.random.choice(np.arange(start=i,stop = i + k_p*L_p,dtype=int),size=k_p,replace=False))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for p in range(i,int(i+k_p*L_p)):\n",
    "\n",
    "        sampling_num_agents += 1\n",
    "        \n",
    "        if p in explore_agents:\n",
    "            choice = i\n",
    "                \n",
    "        else: \n",
    "            choice = a_star\n",
    "            \n",
    "        reward_tuple = bandit.pull(choice)\n",
    "        \n",
    "        if len(regret) > 0:\n",
    "            \n",
    "            regret.append(regret[-1]+p_best - reward_tuple[-1])\n",
    "            \n",
    "        else:\n",
    "            regret.append(p_best - reward_tuple[-1])\n",
    "            \n",
    "        agent.update(choice,reward_tuple[0])\n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampling_num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_radius_parameter = L_p*k_p\n",
    "bandit_agent = BetaAgent(bandit, GreedyPolicy(),ic=True)\n",
    "sim_num_agents = 0\n",
    "for phase in range(num_bandit_rounds):\n",
    "    if phase % 1000 == 0:\n",
    "        print(\"Starting phase \",phase)\n",
    "        \n",
    "    a_star = agent.a_star\n",
    "    bandit_choice = bandit_agent.choose()\n",
    "    explore_agent = np.random.randint(0,L_p)\n",
    "# do a phase of L rounds\n",
    "    for agent_round in range(L_p):\n",
    "        sim_num_agents += 1\n",
    "\n",
    "        if agent_round == explore_agent:\n",
    "\n",
    "            choice = bandit_choice\n",
    "\n",
    "        else:\n",
    "\n",
    "            choice = a_star\n",
    "\n",
    "        reward_tuple = bandit.pull(choice)\n",
    "        if len(regret) > 0:\n",
    "\n",
    "                regret.append(regret[-1]+p_best - reward_tuple[-1])\n",
    "        else:\n",
    "                regret.append(p_best - reward_tuple[-1])\n",
    "        agent.update(choice,reward_tuple[0])\n",
    "        bandit_agent.observe(reward = reward_tuple[0],action_attempt = choice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dca3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# file = open('regret1.pkl', 'wb') \n",
    "# pickle.dump(regret, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9717dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"regret1.pkl\",'rb')\n",
    "regret = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regret[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183582b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = sim_num_agents + sampling_num_agents\n",
    "\n",
    "TSagent = BetaAgent(bandit, GreedyPolicy(),ts=True)\n",
    "regret_ts = []\n",
    "for t in range(n_trials):\n",
    "   \n",
    "\n",
    "\n",
    "    action = TSagent.choose()\n",
    "    reward, is_optimal,mean_reward = bandit.pull(action)\n",
    "    TSagent.observe(reward)\n",
    "\n",
    "    if t%10000 == 0:\n",
    "        print('we are at time: ',t)\n",
    "      \n",
    "\n",
    "    if len(regret_ts) == 0:\n",
    "        regret_ts.append(p_best - mean_reward) \n",
    "    else:\n",
    "        regret_ts.append(regret_ts[-1] + p_best - mean_reward)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e57fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# f = open('regretTS.pkl', 'wb') \n",
    "# pickle.dump(regret_ts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10298d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('regretTS.pkl', 'rb') \n",
    "regret_new = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regret_new[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90993587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(regret_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = k_p + L_p*k_p\n",
    "cExpGap = c*(p_best-p_worst)\n",
    "constant = 1\n",
    "TSRegretBound = lambda x : constant*np.sqrt(n_arms*x*np.log(x)) \n",
    "\n",
    "theoreticalBound = L_p*TSRegretBound(n_trials/L_p) + cExpGap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.ylabel(\"Cummulative Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "\n",
    "plt.plot(regret,label = \"BIC TS\")\n",
    "plt.plot(regret_ts,label = \"TS\")\n",
    "plt.vlines(x=sampling_num_agents,ymin = 0,ymax = regret[sampling_num_agents],color = 'g',label=\"End of Sampling Stage\")\n",
    "\n",
    "plt.axhline(y=theoreticalBound, color='r', linestyle='-',label=\"Theorem 7 Theoretical Bound\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# plt.legend(loc=4)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea95f6",
   "metadata": {},
   "source": [
    "# New Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "num_bandit_rounds = 20000\n",
    "np.random.seed(3)\n",
    "\n",
    "\n",
    "p_array = []\n",
    "\n",
    "p_expected_values = np.linspace(start = 0.9,stop = 0.1, num = n_arms)\n",
    "\n",
    "for i in p_expected_values:\n",
    "    p_array.append(pm.TruncatedNormal.dist(lower=0,upper=1,mu=i,sigma = 0.4).random())\n",
    "    \n",
    "    \n",
    "\n",
    "p_best = np.max(p_array)\n",
    "p_worst = np.min(p_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bandit = BernoulliBandit(k=n_arms,p_array = p_array)\n",
    "rho_p = 1/n_arms\n",
    "tau_p = 1/n_arms\n",
    "\n",
    "L_p  = int(2 + (p_best - p_worst)/(rho_p*tau_p))\n",
    "k_p = 1000\n",
    "\n",
    "print(p_array)\n",
    "print(k_p)\n",
    "print(L_p)\n",
    "print(L_p*num_bandit_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6812c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_num_agents = 0\n",
    "regretUCBIC = []\n",
    "\n",
    "\n",
    "r_1 = np.repeat(-1,k_p)\n",
    "# sampling stage\n",
    "\n",
    "agent = BICAgent(k_p,L_p,n_arms)\n",
    "# sample k_p times from bandit 1\n",
    "for i in range(k_p):\n",
    "    sampling_num_agents += 1\n",
    "    r_1[i] = bandit.pull(0)[0]\n",
    "    agent.update(0,r_1[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(1,n_arms):\n",
    "    \n",
    "    a_star = agent.a_star\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    explore_agents = set(np.random.choice(np.arange(start=i,stop = i + k_p*L_p,dtype=int),size=k_p,replace=False))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for p in range(i,int(i+k_p*L_p)):\n",
    "\n",
    "        sampling_num_agents += 1\n",
    "        \n",
    "        if p in explore_agents:\n",
    "            choice = i\n",
    "                \n",
    "        else: \n",
    "            choice = a_star\n",
    "            \n",
    "        reward_tuple = bandit.pull(choice)\n",
    "        \n",
    "        if len(regretUCBIC) > 0:\n",
    "            \n",
    "            regretUCBIC.append(regretUCBIC[-1]+p_best - reward_tuple[-1])\n",
    "            \n",
    "        else:\n",
    "            regretUCBIC.append(p_best - reward_tuple[-1])\n",
    "            \n",
    "        agent.update(choice,reward_tuple[0])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_radius_parameter = L_p*k_p\n",
    "bandit_agent =  Agent(bandit, UCBPolicy(2,num_bandit_rounds))\n",
    "sim_num_agents = 0\n",
    "for phase in range(num_bandit_rounds):\n",
    "    if phase % 1000 == 0:\n",
    "        print(\"Starting phase \",phase)\n",
    "        \n",
    "    a_star = agent.a_star\n",
    "    bandit_choice = bandit_agent.choose()\n",
    "    explore_agent = np.random.randint(0,L_p)\n",
    "# do a phase of L rounds\n",
    "    for agent_round in range(L_p):\n",
    "        sim_num_agents += 1\n",
    "\n",
    "        if agent_round == explore_agent:\n",
    "\n",
    "            choice = bandit_choice\n",
    "\n",
    "        else:\n",
    "\n",
    "            choice = a_star\n",
    "\n",
    "        reward_tuple = bandit.pull(choice)\n",
    "        if len(regretUCBIC) > 0:\n",
    "\n",
    "                regretUCBIC.append(regretUCBIC[-1]+p_best - reward_tuple[-1])\n",
    "        else:\n",
    "                regretUCBIC.append(p_best - reward_tuple[-1])\n",
    "        agent.update(choice,reward_tuple[0])\n",
    "        bandit_agent.observe(reward = reward_tuple[0],action_attempt = choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = len(regretUCBIC)\n",
    "\n",
    "UCBagent = Agent(bandit, UCBPolicy(2,num_bandit_rounds))\n",
    "regret_ucb = []\n",
    "for t in range(n_trials):\n",
    "   \n",
    "\n",
    "\n",
    "    action = UCBagent.choose()\n",
    "    reward, is_optimal,mean_reward = bandit.pull(action)\n",
    "    UCBagent.observe(reward)\n",
    "\n",
    "    if t%10000 == 0:\n",
    "        print('we are at time: ',t)\n",
    "      \n",
    "\n",
    "    if len(regret_ucb) == 0:\n",
    "        regret_ucb.append(p_best - mean_reward) \n",
    "    else:\n",
    "        regret_ucb.append(regret_ucb[-1] + p_best - mean_reward)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumGaps = 0\n",
    "\n",
    "for i in p_array:\n",
    "    if i != p_best:\n",
    "        sumGaps += 1/(p_best - i)\n",
    "        \n",
    "constant = 1\n",
    "\n",
    "UCBRegretBound = lambda x : constant*np.log(x)*sumGaps\n",
    "\n",
    "c = k_p + L_p*k_p\n",
    "cExpGap = c*(p_best-p_worst)\n",
    "\n",
    "theoreticalBound = L_p*UCBRegretBound((len(regretUCBIC))/L_p) + cExpGap\n",
    "print(theoreticalBound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93094e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.ylabel(\"Cummulative Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "\n",
    "plt.plot(regretUCBIC,label = \"BIC UCB1\")\n",
    "plt.plot(regret_ucb,label = \"UCB1\")\n",
    "plt.vlines(x=sampling_num_agents,ymin = 0,ymax = regretUCBIC[sampling_num_agents],color = 'g',label=\"End of Sampling Stage\")\n",
    "\n",
    "plt.axhline(y=theoreticalBound, color='r', linestyle='-',label=\"Theorem 7 Theoretical Bound\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# plt.legend(loc=4)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
