{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c366286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from bandits.environment import Environment\n",
    "from bandits.bandit import GaussianBandit\n",
    "from bandits.bandit import BernoulliBandit\n",
    "from bandits.bandit import TruncatedGaussianBandit\n",
    "import math\n",
    "from bandits.agent import Agent, GradientAgent\n",
    "from bandits.policy import (EpsilonGreedyPolicy, GreedyPolicy, UCBPolicy,\n",
    "                            ExploreFirstPolicy,\n",
    "                            SoftmaxPolicy)\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from bandits.agent import Agent, BetaAgent\n",
    "import seaborn as sns\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0edaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "sigma = 1\n",
    "sigma_means = 0.3\n",
    "\n",
    "n_arms = 10\n",
    "bandit = TruncatedGaussianBandit(n_arms,sigma=sigma,sigma_means = sigma_means)\n",
    "# bandit = BernoulliBandit(k=n_arms)\n",
    "n_trials = 20000\n",
    "n_experiments = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99375ade",
   "metadata": {},
   "source": [
    "## Explore first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc539524",
   "metadata": {},
   "source": [
    "## High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agents = [\n",
    "        Agent(bandit, ExploreFirstPolicy(N,n_arms)),\n",
    "      \n",
    "    ]\n",
    "env = Environment(bandit, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a84e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, optimal,regret = env.run(n_trials, n_experiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_reward(scores,label = \"Reward\")\n",
    "plt.figure()\n",
    "env.plot_regret(regret,label = \"Cumulative Regret\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4246",
   "metadata": {},
   "source": [
    "## Low Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9014363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.01\n",
    "bandit = TruncatedGaussianBandit(n_arms,sigma=sigma)\n",
    "# bandit = BernoulliBandit(k=n_arms)\n",
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "agents = [\n",
    "        Agent(bandit, ExploreFirstPolicy(N,n_arms)),\n",
    "      \n",
    "    ]\n",
    "env = Environment(bandit, agents)\n",
    "\n",
    "scores, optimal,regret = env.run(n_trials, n_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_reward(scores,label = \"Reward\")\n",
    "plt.figure()\n",
    "env.plot_regret(regret,label = \"Cumulative Regret\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bd935",
   "metadata": {},
   "source": [
    "# e-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 100\n",
    "n_trials = 1000\n",
    "n_experiments = 100\n",
    "\n",
    "\n",
    "p_array = [np.random.normal(0.5,0.1) for i in range(n_arms)]\n",
    "\n",
    "\n",
    "\n",
    "bandit = BernoulliBandit(k=n_arms,p_array = p_array)\n",
    "\n",
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "print(N*n_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7deee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [Agent(bandit, ExploreFirstPolicy(N,n_arms)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.01)),\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score1, optimal1,regret1 = env.run(n_trials, n_experiments)\n",
    "scores1,optimal1,regret1 = score1/n_experiments,optimal1/n_experiments,regret1/n_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10000\n",
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "agents = [Agent(bandit, ExploreFirstPolicy(N,n_arms)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.01)),\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score2, optimal2,regret2 = env.run(n_trials, n_experiments)\n",
    "scores2,optimal2,regret2 = score2/n_experiments,optimal2/n_experiments,regret2/n_experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100000\n",
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "agents = [Agent(bandit, ExploreFirstPolicy(N,n_arms)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.01)),\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score3, optimal3,regret3 = env.run(n_trials, n_experiments)\n",
    "scores3,optimal3,regret3 = score3/n_experiments,optimal3/n_experiments,regret3/n_experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1581433",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11.0, 10)\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "plt.subplot(131)\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret1)\n",
    "plt.legend(agents, loc=2)\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret2)\n",
    "plt.legend(agents, loc=2)\n",
    "\n",
    "plt.subplot(133)\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret3)\n",
    "plt.legend(agents, loc=2)\n",
    "\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100000\n",
    "n_experiments = 1\n",
    "agents = [\n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)), \n",
    "          Agent(bandit, UCBPolicy(2,n_trials))\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score3, optimal3,regret3 = env.run(n_trials, n_experiments)\n",
    "scores3,optimal3,regret3 = score3/n_experiments,optimal3/n_experiments,regret3/n_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11.0, 10)\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret3)\n",
    "plt.legend(agents, loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000000\n",
    "n_experiments = 1\n",
    "agents = [\n",
    "          BetaAgent(bandit, GreedyPolicy()), \n",
    "          Agent(bandit, UCBPolicy(2,n_trials))\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score4, optimal4,regret4 = env.run(n_trials, n_experiments)\n",
    "scores4,optimal4,regret4 = score4/n_experiments,optimal4/n_experiments,regret4/n_experiments\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000000\n",
    "n_experiments = 1\n",
    "N = math.ceil((n_trials/n_arms)**(2/3)*math.log(n_trials**(1/3)))\n",
    "agents = [\n",
    "          BetaAgent(bandit, GreedyPolicy()), \n",
    "        Agent(bandit, ExploreFirstPolicy(N,n_arms)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)),\n",
    "          Agent(bandit, UCBPolicy(2,n_trials))\n",
    "         \n",
    "         ]\n",
    "              \n",
    "              \n",
    "env = Environment(bandit, agents)\n",
    "score5, optimal5,regret5 = env.run(n_trials, n_experiments)\n",
    "scores5,optimal5,regret5 = score5/n_experiments,optimal5/n_experiments,regret5/n_experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52012d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11.0, 10)\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret4)\n",
    "plt.legend(agents, loc=2)\n",
    "\n",
    "agents = [\n",
    "          BetaAgent(bandit, GreedyPolicy()), \n",
    "        Agent(bandit, ExploreFirstPolicy(N,n_arms)), \n",
    "          Agent(bandit,EpsilonGreedyPolicy(epsilon = 0.1)),\n",
    "          Agent(bandit, UCBPolicy(2,n_trials))\n",
    "         \n",
    "         ]\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "\n",
    "plt.ylabel(\"Average Regret\")\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "\n",
    "plt.plot(regret5)\n",
    "plt.legend(agents, loc=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
